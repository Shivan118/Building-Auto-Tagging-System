{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import libraries and dataset\n",
    "2. Inspect data\n",
    "3. Clean and pre-process data\n",
    "4. Reshape Target variable\n",
    "5. Extract features from data\n",
    "6. Build model for multilabel classification\n",
    "7. Make predictions and evaluate model on validation set\n",
    "8. Define inference function for new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df_questions = pd.read_hdf('auto_tagging_data_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41763</th>\n",
       "      <td>92185</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?</td>\n",
       "      <td>&lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Approximate a target distribution $p$ using an importance sample $S$ fro...</td>\n",
       "      <td>[sampling, mcmc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>179778</td>\n",
       "      <td>optimization approach in logistic regression</td>\n",
       "      <td>&lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...</td>\n",
       "      <td>[machine-learning, logistic, classification, optimization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37183</th>\n",
       "      <td>168679</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model</td>\n",
       "      <td>&lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...</td>\n",
       "      <td>[regression, survival, cox-model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55932</th>\n",
       "      <td>144226</td>\n",
       "      <td>Moments and density tails</td>\n",
       "      <td>&lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does there exist a methodology to...</td>\n",
       "      <td>[probability, pdf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47629</th>\n",
       "      <td>142745</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables?</td>\n",
       "      <td>&lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.&lt;/p&gt;\\n</td>\n",
       "      <td>[variance, covariance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49639</th>\n",
       "      <td>195347</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?</td>\n",
       "      <td>&lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...</td>\n",
       "      <td>[machine-learning, nonlinear-regression]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "41763   92185   \n",
       "4245   179778   \n",
       "37183  168679   \n",
       "55932  144226   \n",
       "47629  142745   \n",
       "49639  195347   \n",
       "\n",
       "                                                                                                   Title  \\\n",
       "41763                  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?   \n",
       "4245                                                        optimization approach in logistic regression   \n",
       "37183                             Consequences of violating proportional hazards assumption in Cox model   \n",
       "55932                                                                          Moments and density tails   \n",
       "47629            What is the demonstration of the variance of the difference of two dependent variables?   \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?   \n",
       "\n",
       "                                                                                                                                                                                                          Body  \\\n",
       "41763  <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n<ol>\\n<li>Approximate a target distribution $p$ using an importance sample $S$ fro...   \n",
       "4245   <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...   \n",
       "37183  <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...   \n",
       "55932  <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does there exist a methodology to...   \n",
       "47629                <p>I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.</p>\\n   \n",
       "49639  <p>I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...   \n",
       "\n",
       "                                                             Tags  \n",
       "41763                                            [sampling, mcmc]  \n",
       "4245   [machine-learning, logistic, classification, optimization]  \n",
       "37183                           [regression, survival, cox-model]  \n",
       "55932                                          [probability, pdf]  \n",
       "47629                                      [variance, covariance]  \n",
       "49639                    [machine-learning, nonlinear-regression]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.sample(6, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41763</th>\n",
       "      <td>92185</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?</td>\n",
       "      <td>&lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Approximate a target distribution $p$ using an importance sample $S$ fro...</td>\n",
       "      <td>[sampling, mcmc]</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? &lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>179778</td>\n",
       "      <td>optimization approach in logistic regression</td>\n",
       "      <td>&lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...</td>\n",
       "      <td>[machine-learning, logistic, classification, optimization]</td>\n",
       "      <td>optimization approach in logistic regression &lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37183</th>\n",
       "      <td>168679</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model</td>\n",
       "      <td>&lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...</td>\n",
       "      <td>[regression, survival, cox-model]</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model &lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55932</th>\n",
       "      <td>144226</td>\n",
       "      <td>Moments and density tails</td>\n",
       "      <td>&lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does there exist a methodology to...</td>\n",
       "      <td>[probability, pdf]</td>\n",
       "      <td>Moments and density tails &lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47629</th>\n",
       "      <td>142745</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables?</td>\n",
       "      <td>&lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.&lt;/p&gt;\\n</td>\n",
       "      <td>[variance, covariance]</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables? &lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49639</th>\n",
       "      <td>195347</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?</td>\n",
       "      <td>&lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...</td>\n",
       "      <td>[machine-learning, nonlinear-regression]</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? &lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "41763   92185   \n",
       "4245   179778   \n",
       "37183  168679   \n",
       "55932  144226   \n",
       "47629  142745   \n",
       "49639  195347   \n",
       "\n",
       "                                                                                                   Title  \\\n",
       "41763                  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?   \n",
       "4245                                                        optimization approach in logistic regression   \n",
       "37183                             Consequences of violating proportional hazards assumption in Cox model   \n",
       "55932                                                                          Moments and density tails   \n",
       "47629            What is the demonstration of the variance of the difference of two dependent variables?   \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?   \n",
       "\n",
       "                                                                                                                                                                                                          Body  \\\n",
       "41763  <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n<ol>\\n<li>Approximate a target distribution $p$ using an importance sample $S$ fro...   \n",
       "4245   <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...   \n",
       "37183  <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...   \n",
       "55932  <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does there exist a methodology to...   \n",
       "47629                <p>I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.</p>\\n   \n",
       "49639  <p>I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...   \n",
       "\n",
       "                                                             Tags  \\\n",
       "41763                                            [sampling, mcmc]   \n",
       "4245   [machine-learning, logistic, classification, optimization]   \n",
       "37183                           [regression, survival, cox-model]   \n",
       "55932                                          [probability, pdf]   \n",
       "47629                                      [variance, covariance]   \n",
       "49639                    [machine-learning, nonlinear-regression]   \n",
       "\n",
       "                                                                                                                                                                                                          Text  \n",
       "41763  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n...  \n",
       "4245   optimization approach in logistic regression <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...  \n",
       "37183  Consequences of violating proportional hazards assumption in Cox model <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...  \n",
       "55932  Moments and density tails <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does th...  \n",
       "47629  What is the demonstration of the variance of the difference of two dependent variables? <p>I know that the variance of the difference of two independent variables is the sum of variances, and I ca...  \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? <p>I was trying to understand how much data I would need compared to the number of parameters (and...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine title and body\n",
    "df_questions['Text'] = df_questions[\"Title\"] + \" \" + df_questions[\"Body\"]\n",
    "df_questions.sample(6, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Two Cultures: statistics vs. machine learning? <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statisti...\n",
       "1    Forecasting demographic census <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census ...\n",
       "2                             Bayesian and frequentist reasoning in plain English <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n\n",
       "3    What is the meaning of p values and t values in statistical tests? <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk b...\n",
       "4    Examples for teaching: Correlation does not mean causation <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustra...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove html tags and url links\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove everything alphabets\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    # remove whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Text'].apply(lambda x: clean_text(x))\n",
    "df_questions['Text'] = df_questions['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41763</th>\n",
       "      <td>92185</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? &lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n...</td>\n",
       "      <td>[sampling, mcmc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>179778</td>\n",
       "      <td>optimization approach in logistic regression &lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...</td>\n",
       "      <td>[machine-learning, logistic, classification, optimization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37183</th>\n",
       "      <td>168679</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model &lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...</td>\n",
       "      <td>[regression, survival, cox-model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55932</th>\n",
       "      <td>144226</td>\n",
       "      <td>Moments and density tails &lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does th...</td>\n",
       "      <td>[probability, pdf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47629</th>\n",
       "      <td>142745</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables? &lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I ca...</td>\n",
       "      <td>[variance, covariance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49639</th>\n",
       "      <td>195347</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? &lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and...</td>\n",
       "      <td>[machine-learning, nonlinear-regression]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "41763   92185   \n",
       "4245   179778   \n",
       "37183  168679   \n",
       "55932  144226   \n",
       "47629  142745   \n",
       "49639  195347   \n",
       "\n",
       "                                                                                                                                                                                                          Text  \\\n",
       "41763  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n...   \n",
       "4245   optimization approach in logistic regression <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...   \n",
       "37183  Consequences of violating proportional hazards assumption in Cox model <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...   \n",
       "55932  Moments and density tails <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does th...   \n",
       "47629  What is the demonstration of the variance of the difference of two dependent variables? <p>I know that the variance of the difference of two independent variables is the sum of variances, and I ca...   \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? <p>I was trying to understand how much data I would need compared to the number of parameters (and...   \n",
       "\n",
       "                                                             Tags  \n",
       "41763                                            [sampling, mcmc]  \n",
       "4245   [machine-learning, logistic, classification, optimization]  \n",
       "37183                           [regression, survival, cox-model]  \n",
       "55932                                          [probability, pdf]  \n",
       "47629                                      [variance, covariance]  \n",
       "49639                    [machine-learning, nonlinear-regression]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions[['Id', 'Text', 'Tags']].sample(6, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_stopwords(text):\n",
    "    clean_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text_clean'] = df_questions['Text'].apply(lambda x: strip_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "\n",
    "multilabel_binarizer.fit(df_questions['Tags'])\n",
    "\n",
    "# transform target variable (\"Tags\")\n",
    "Y = multilabel_binarizer.transform(df_questions['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76365, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_questions['Text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and validation set\n",
    "x_train_tfidf, x_val_tfidf, y_train_tfidf, y_val_tfidf = train_test_split(X_tfidf, Y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "clf = OneVsRestClassifier(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticsvidhya/miniconda3/envs/datahack/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model on train data\n",
    "clf.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Performane Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for validation set\n",
    "y_pred = clf.predict(x_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# print prediction\n",
    "print(y_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prediction',), ('distributions', 'mean', 'variance'), ()]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_binarizer.inverse_transform(y_pred)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multilabel-indicator but average='binary'. Please choose another average setting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4d85a233f608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/datahack/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    718\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    719\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datahack/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    832\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    835\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datahack/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m-> 1047\u001b[0;31m                              \"choose another average setting.\" % y_type)\n\u001b[0m\u001b[1;32m   1048\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multilabel-indicator but average='binary'. Please choose another average setting."
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "f1_score(y_val_tfidf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02040816, 0.6080402 , 0.60983607, 0.38918919, 0.5776699 ,\n",
       "       0.13636364, 0.35433071, 0.66122449, 0.51111111, 0.2246696 ,\n",
       "       0.51282051, 0.44018059, 0.71470588, 0.18461538, 0.58695652,\n",
       "       0.5596222 , 0.30337079, 0.51572327, 0.65030675, 0.07960199,\n",
       "       0.29577465, 0.4009324 , 0.0130719 , 0.40993789, 0.28172043,\n",
       "       0.03296703, 0.18367347, 0.26900585, 0.23602484, 0.52849741,\n",
       "       0.35379061, 0.51936219, 0.40559441, 0.30769231, 0.35631155,\n",
       "       0.04225352, 0.5       , 0.04347826, 0.28571429, 0.04166667,\n",
       "       0.63688213, 0.38063439, 0.01104972, 0.46096654, 0.53134328,\n",
       "       0.37241379, 0.16494845, 0.46994536, 0.52903226, 0.        ,\n",
       "       0.21333333, 0.        , 0.35483871, 0.23312883, 0.31578947,\n",
       "       0.18875502, 0.15151515, 0.75644699, 0.05263158, 0.31336406,\n",
       "       0.36333333, 0.19487179, 0.45962733, 0.49101796, 0.13492063,\n",
       "       0.5060241 , 0.77272727, 0.26666667, 0.46153846, 0.05376344,\n",
       "       0.05970149, 0.39434276, 0.3030303 , 0.41975309, 0.57242018,\n",
       "       0.27777778, 0.65551839, 0.05442177, 0.32388664, 0.49249249,\n",
       "       0.07633588, 0.4973822 , 0.36734694, 0.28971963, 0.3372093 ,\n",
       "       0.26303855, 0.18421053, 0.45454545, 0.42741935, 0.27058824,\n",
       "       0.37554585, 0.22926094, 0.16806723, 0.33548387, 0.67716535,\n",
       "       0.63797468, 0.22105263, 0.        , 0.63327769, 0.37303371])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_tfidf, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34361729591925294"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_score(y_val_tfidf, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4312861087264206"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate performance\n",
    "f1_score(y_val_tfidf, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34361729591925294"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_tfidf, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "y_pred_prob = clf.predict_proba(x_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45988232147633057"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set threshold value\n",
    "t = 0.45\n",
    "\n",
    "# convert to integers\n",
    "y = (y_pred_prob >= t).astype(int)\n",
    "f1_score(y_val_tfidf, y, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tags(q):\n",
    "    q = clean_text(q)\n",
    "    q = q.lower()\n",
    "    q = strip_stopwords(q)\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    q_pred = clf.predict(q_vec)\n",
    "    return multilabel_binarizer.inverse_transform(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 'regression')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give new question\n",
    "new_q = \"Regression line in ggplot doesn't match computed regression Im using R and created a chart using ggplot2. I then create a regression so I can make some predicitions I pass my data frame of to the predict function predict(regression, Measures) I'd expect the predictions to be the same as if I used the regression line on the chart, but they aren't the same. Why would this be the case? Is there a setting in ggplot or is my expectation incorrect?\"\n",
    "\n",
    "# get tags\n",
    "infer_tags(new_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
